{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95abe01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T08:12:07.943165Z",
     "start_time": "2024-04-12T08:12:07.926767Z"
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "환경과의 상호작용을 통해 축적되는 경험적인 데이터를 바탕으로 모델을 자동적으로 구축하고 스스로 성능을 향상시키는 시스템.  \n",
    "control task(제어) 문제를 풀기 위한 머신러닝 방법론  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99cbb2",
   "metadata": {},
   "source": [
    "## Component\n",
    "> - Q(가치함수, 목적함수) : 풀어내야 하는 문제에 따라 최적화 시켜야 할 함수, 문제에 따라 최대화\n",
    "> - agent(플레이어) : 환경에 놓여져 환경과의 상호작용 및 상태 측정을 하며 변화하는 상태에 따라 action을 수행하는 주체 \n",
    "> - Environment(환경) : agent가 놓여져 있는 시공간 시계열에 따라 변화하는 상태 공간 \n",
    "> - State(상태) : agent가 action을 취한 시점의 환경의 현재 상황, 시계열 중 특정 시점\n",
    "> - Policy(정책) : agnet가 어떤 action을 취해야 하는지에 대한 가이드\n",
    "> - Action(행동) : 환경에 놓여진 Agent가 선택할 수 있는 행동 옵션\n",
    "> - Reward(보상) : Agent가 선택한 Action에 따라 주어지는 보상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2303f",
   "metadata": {},
   "source": [
    "<img src=\"./image/01.jpg\">\n",
    "\n",
    "출처 : https://medium.com/analytics-vidhya/basic-terminology-reinforcement-learning-2357fd5f0e51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6993a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T08:20:43.765417Z",
     "start_time": "2024-04-12T08:20:43.761079Z"
    }
   },
   "source": [
    "## 문제정의\n",
    "\n",
    "### 밴디트(강도) 문제\n",
    "> - a : 플레이어\n",
    "> - Q : 코인 갯수를 가장 많이 얻을 수 있는 방법으로 기대값이 가장 높은 슬롯머신을 선택\n",
    "> - E : 10대의 다른 슬롯머신\n",
    "> - A : 슬롯머신 한 대를 선택하고 1게임을 진행\n",
    "> - R : 1게임을 진행 했을 때의 획득 코인 갯수\n",
    "\n",
    "### 게임 자동화\n",
    "> - a : 유저\n",
    "> - Q : 스코어 최대화를 통해 가장 게임을 잘 할 수 있는 방법을 찾음\n",
    "> - E : 게임 환경 (벽돌깨기, 테트리스, 비행기 게임)\n",
    "> - A : 수행 한 유저의 동작\n",
    "> - R : 유저의 동작에 따라 확인 한 게임 내 스코어\n",
    "\n",
    "### 마케팅 채널 선택 문제\n",
    "> - a : 마케팅 회사\n",
    "> - Q : R(매출)을 극대화 하기 위한 가장 효율이 좋은 마케팅 채널을 선택\n",
    "> - E : 온라인, TV, 라디오, 오프라인 채널 등\n",
    "> - A : 프로모션 진행\n",
    "> - R : 모든 프로모션 채널에서 발생하는 매출\n",
    "\n",
    "### 이족 보행 로봇의 움직임 제어를 위한 강화학습 문제를 각 구성요소로 구분한다면\n",
    "> - a : 로보트\n",
    "> - Q : 보상을 최대화하여 무게중심을 잘 잡을 수 있는 action을 유도\n",
    "> - E : 트래드밀\n",
    "> - A : 로봇의 발목, 무릎, 팔, 골반 움직임의 각도 등\n",
    "> - R : 무게중심을 잘 잡고 있는 경우 + 보상 / 무게중심을 잡지 못해 넘어질 경우 - 패널티를 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95abae1",
   "metadata": {},
   "source": [
    "## Mathatical explainaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79639c13",
   "metadata": {},
   "source": [
    "# $$q(A) = \\mathbb{E}[R|A]$$\n",
    "\n",
    "agent가 A라는 액션을 취했을 경우의 조건이 주어졌을 때의 reward가 곧 action A의 Q(행동가치 함수)가 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
